---
description: >-
  https://people.mpi-inf.mpg.de/~mehlhorn/SeminarEvolvability/ValiantLearnable.pdf
---

# A Theory of the Learnable

This paper revolves around the idea of what could be called as learning? learnable tasks? and Programming. So it was succinctly defined as the _phenomenon of knowledge acquisition in the absence of explicit programming._ 

 From a computational point of view, how can this phenomenon be studied. Author presents the framework/methodology as following

* [ ] Information Gathering mechanism
* [ ] Learning protocol that can be terminated in a reasonable number of steps
* [ ] Use models to program and learn more about class of concepts and develop new algorithms

> The problem is to discover good models that are interesting to study for their own sake and that promise to be relevant both to explaining human experience and to building devices that can learn.

Like some elements of human learning seems genetically programmed with a lot of randomness while other type of learning like recognizing tables seems memorized. And also we can't really say what algorithm we use in recognizing chairs and tables. 

**Probably Approximate learning**: Perhaps the main technical discovery contained in the paper is that the notion of learning from data examples and probability distributions that generate those data, . With this probabilistic notion of learning highly convergent learning is possible for whole classes of Boolean functions. 

The main gist of paper was : Introduce The Hypothesis or "_Candidate function_". See how good it generalized for data examples and formulate how good/bad  in terms of probabilistic concepts.The intuition is "Hypothesis" generalizing well for lots and lots of training data are unlikely to be wrong and more likely to be "Probably Approximate learning".

 This appears to distinguish this approach from more traditional ones where learning is seen as a process of "inducing" some general rule from information. 







